# 支持向量机 (Support Vector Machine)

## 算法概述

支持向量机是一种监督学习算法，通过寻找最优超平面来实现分类或回归，具有良好的泛化能力和在高维空间的有效性。

## 核心原理

1. **最优超平面**：寻找最大化分类间隔的超平面
2. **支持向量**：位于间隔边界上的训练样本
3. **核技巧**：将数据映射到高维特征空间
4. **软间隔**：允许一定程度的分类错误

## 算法流程

```
选择核函数 → 构造约束优化问题 → 使用SMO算法求解 → 得到决策函数
```

## 代码结构

- `SimpleSVM.java`: 简化的线性SVM实现
- `SVMExample.java`: 二分类应用示例

## 经典应用

### 1. 文本分类
- **垃圾邮件过滤**: 识别垃圾邮件
- **情感分析**: 正负面情感分类
- **主题分类**: 文档自动分类

### 2. 图像识别
- **人脸识别**: 人脸检测和识别
- **手写数字识别**: 数字字符识别
- **物体检测**: 目标物体定位

### 3. 生物信息学
- **基因分类**: DNA序列分类
- **蛋白质分类**: 蛋白质功能预测
- **药物发现**: 分子活性预测

### 4. 金融领域
- **信用评估**: 信用风险评估
- **欺诈检测**: 异常交易检测
- **市场预测**: 股价走势预测

### 5. 其他应用
- **医疗诊断**: 疾病预测和诊断
- **语音识别**: 语音信号分类
- **推荐系统**: 用户偏好预测

## 核函数

### 线性核
```
K(x,y) = x·y
```
- 适用于线性可分问题

### 多项式核
```
K(x,y) = (x·y + c)^d
```
- 适用于非线性问题

### RBF核 (高斯核)
```
K(x,y) = exp(-γ||x-y||²)
```
- 最常用的核函数
- 适用于大多数非线性问题

### Sigmoid核
```
K(x,y) = tanh(γ(x·y) + c)
```
- 类似于神经网络

## 参数说明

- **惩罚参数 C**: 控制软间隔的惩罚程度
- **核参数 γ**: RBF核的参数，影响决策边界复杂度
- **多项式次数 d**: 多项式核的次数

## 优势

- 理论基础扎实
- 泛化能力强
- 对高维数据有效
- 避免维度灾难
- 内存效率高

## 局限性

- 训练时间长
- 参数选择困难
- 对噪声敏感
- 难以解释结果
- 不适合大规模数据

## 使用建议

1. **数据预处理**:
   - 特征标准化
   - 处理缺失值
   - 特征选择

2. **参数选择**:
   - 使用交叉验证
   - 网格搜索最优参数
   - 考虑计算成本

3. **核函数选择**:
   - 从RBF核开始
   - 根据问题特点选择
   - 避免过度复杂

## 运行示例

```bash
cd src/algorithm/IA/SVM
javac *.java
java SVMExample
```

## 数学基础

### 硬间隔SVM
```
min  ||w||²/2
s.t.  y_i(w·x_i + b) ≥ 1
```

### 软间隔SVM
```
min  ||w||²/2 + C Σ ξ_i
s.t.  y_i(w·x_i + b) ≥ 1 - ξ_i
```

### 对偶问题
```
max Σ α_i - (1/2) Σ α_i α_j y_i y_j K(x_i,x_j)
s.t.  0 ≤ α_i ≤ C, Σ α_i y_i = 0
```

## 变体算法

1. **ν-SVM**: 自动确定支持向量比例
2. **多类SVM**: 一对多、一对一策略
3. **回归SVM**: 支持向量回归(SVR)
4. **结构化SVM**: 结构化输出预测

## 参考文献

1. Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning.
2. Burges, C. J. (1998). A tutorial on support vector machines for pattern recognition.
3. Schölkopf, B., & Smola, A. J. (2002). Learning with Kernels.
