# 人工神经网络 (Artificial Neural Networks)

## 算法概述

人工神经网络是一种模拟生物神经系统的计算模型，通过多层神经元及其连接来处理和学习复杂模式。

## 核心原理

1. **神经元模型**：模拟生物神经元的信息处理
2. **网络结构**：层级连接的神经元网络
3. **学习算法**：通过训练数据调整连接权重
4. **激活函数**：引入非线性变换能力

## 网络结构

```
输入层 → 隐藏层 → 输出层
```

## 代码结构

- `NeuralNetwork.java`: 多层感知机实现
- `XORExample.java`: XOR问题训练示例

## 经典应用

### 1. 模式识别
- **图像识别**: 手写数字识别、物体检测
- **语音识别**: 语音转文字、说话人识别
- **文本分类**: 情感分析、垃圾邮件过滤

### 2. 预测问题
- **时间序列预测**: 股票价格、天气预报
- **需求预测**: 销售预测、库存管理
- **风险评估**: 信用评分、欺诈检测

### 3. 控制系统
- **机器人控制**: 路径规划、运动控制
- **过程控制**: 工业自动化、质量控制
- **游戏AI**: 博弈策略、决策制定

### 4. 数据挖掘
- **聚类分析**: 用户分群、异常检测
- **特征学习**: 降维、特征提取
- **推荐系统**: 商品推荐、内容推荐

### 5. 其他领域
- **医学诊断**: 疾病预测、影像分析
- **金融分析**: 市场分析、投资决策
- **自然语言处理**: 机器翻译、问答系统

## 网络类型

### 前馈神经网络
- 信息单向传播
- 包括多层感知机(MLP)

### 循环神经网络(RNN)
- 处理序列数据
- 具有记忆能力

### 卷积神经网络(CNN)
- 图像处理优异
- 局部连接和权重共享

## 学习算法

### 监督学习
- **反向传播算法**: 基于梯度的学习
- **随机梯度下降**: 参数优化方法

### 无监督学习
- **自编码器**: 数据压缩和重建
- **受限玻尔兹曼机**: 概率生成模型

## 参数说明

- **学习率 (learningRate)**: 控制权重更新幅度，通常0.001-0.1
- **隐藏层大小 (hiddenSize)**: 隐藏层神经元数量
- **训练轮数 (epochs)**: 训练迭代次数
- **批大小 (batchSize)**: 每次更新的样本数

## 激活函数

### Sigmoid函数
```
σ(x) = 1 / (1 + e^(-x))
```
- 输出范围: (0, 1)
- 适用于二分类输出

### Tanh函数
```
tanh(x) = 2σ(2x) - 1
```
- 输出范围: (-1, 1)
- 零中心化

### ReLU函数
```
ReLU(x) = max(0, x)
```
- 计算效率高
- 避免梯度消失

## 训练技巧

### 数据预处理
- 特征标准化
- 数据增强
- 样本平衡

### 正则化技术
- L1/L2正则化
- Dropout
- 批量归一化

### 优化算法
- Adam优化器
- RMSProp
- AdaGrad

## 优势

- 非线性建模能力强
- 自适应学习能力
- 泛化能力好
- 并行处理能力强

## 局限性

- 训练时间长
- 参数调节困难
- 解释性差
- 对数据依赖性强

## 使用建议

1. **网络设计**:
   - 输入层: 根据特征数量
   - 隐藏层: 通常1-3层
   - 输出层: 根据任务类型

2. **训练策略**:
   - 使用交叉验证
   - 监控训练过程
   - 防止过拟合

3. **性能优化**:
   - 选择合适优化器
   - 调整学习率策略
   - 使用正则化技术

## 运行示例

```bash
cd src/algorithm/IA/ANN
javac *.java
java XORExample
```

## 深度学习扩展

### 深度神经网络
- 更多隐藏层
- 更复杂的网络结构

### 注意力机制
- Transformer架构
- 长距离依赖建模

### 生成对抗网络(GAN)
- 生成模型
- 图像合成

## 数学基础

### 损失函数
- 均方误差 (MSE)
- 交叉熵损失
- 对数损失

### 梯度下降
```
w = w - η * ∂L/∂w
```

### 反向传播
- 链式法则计算梯度
- 误差逐层传播

## 参考文献

1. Rumelhart, D. E., et al. (1986). Learning representations by back-propagating errors.
2. LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition.
3. Hinton, G. E. (2006). Reducing the dimensionality of data with neural networks.
4. Goodfellow, I., et al. (2016). Deep Learning.
